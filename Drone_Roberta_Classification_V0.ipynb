{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Drone Roberta Classification V0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLYmHMeXn1UNp7ARcRZAlt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yili-Zhang/hello-world/blob/master/Drone_Roberta_Classification_V0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXLuzaHT-aRx",
        "outputId": "77a2f965-2fc2-4834-c4a1-4796bb2228f5"
      },
      "source": [
        "\"\"\"\n",
        "Install this\n",
        "\"\"\"\n",
        "\n",
        "!pip install pytorch-transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 19.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 28.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 51.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.18.5)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/31/4d4861a90d66c287a348fd17eaefefcdc2e859951cab9884b555923f046d/boto3-1.16.23-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 55.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.7.0+cu101)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.17.0)\n",
            "Collecting botocore<1.20.0,>=1.19.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/49/c8c99477416fdebb59078bda624acc5b3c7008f891c60d56d6ff1570d83e/botocore-1.19.23-py2.py3-none-any.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 51.6MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.7)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.23->boto3->pytorch-transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=dc29c7dd645985d250b4f52df546386eb5a63cdab51343577904f1074d374d3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: botocore 1.19.23 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, sacremoses, jmespath, botocore, s3transfer, boto3, pytorch-transformers\n",
            "Successfully installed boto3-1.16.23 botocore-1.19.23 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWO3rwF26TB1",
        "outputId": "dbd6de9d-eacf-4f03-a980-80a52976284e"
      },
      "source": [
        "\"\"\"\n",
        "Import Libraries\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "## Torch Modules\n",
        "import torch\n",
        "from pytorch_transformers import RobertaTokenizer\n",
        "from pytorch_transformers import RobertaForSequenceClassification, RobertaConfig\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ9faZKZ64PL",
        "outputId": "70780055-2fef-4a40-a208-80e0201edfca"
      },
      "source": [
        "\"\"\"\n",
        "Make Sure Cuda is Available\n",
        "\"\"\"\n",
        "\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHPfQuOd9e_o",
        "outputId": "74cb9490-8547-40cd-a52c-13ac8c6c462e"
      },
      "source": [
        "\"\"\"\n",
        "Loading Configurations\n",
        "\"\"\"\n",
        "label_to_ix = {'goForward': 0, 'goToRoom': 1}\n",
        "config = RobertaConfig.from_pretrained('roberta-base')\n",
        "config.num_labels = len(list(label_to_ix.values()))\n",
        "\n",
        "\"\"\"\n",
        "Loading Pretrained tokenizer and instantiating the model from settings in config\n",
        "\"\"\"\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaForSequenceClassification(config)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 898823/898823 [00:00<00:00, 26871690.19B/s]\n",
            "100%|██████████| 456318/456318 [00:00<00:00, 18624037.02B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQCIZTaB7QLB",
        "outputId": "b3fa3103-b747-448f-fc5b-55382b585f68"
      },
      "source": [
        "\"\"\"\n",
        "Model Path\n",
        "\"\"\"\n",
        "model_path = 'drive/My Drive/AutoDrone-NLP-Classification-Engine-V0/roberta_state_dict_ef8ac38a-77c6-4139-bed1-70dcab601390.pth'\n",
        "\n",
        "'''\n",
        "Loading State dictionaries\n",
        "'''\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.cuda()\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mM5JgH-7ouZ"
      },
      "source": [
        "\"\"\"\n",
        "Important Feature Engineering\n",
        "\"\"\"\n",
        "\n",
        "def prepare_features(seq_1, max_seq_length = 300, \n",
        "             zero_pad = False, include_CLS_token = True, include_SEP_token = True):\n",
        "    ## Tokenzine Input\n",
        "    tokens_a = tokenizer.tokenize(seq_1)\n",
        "\n",
        "    ## Truncate\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "        tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
        "    ## Initialize Tokens\n",
        "    tokens = []\n",
        "    if include_CLS_token:\n",
        "        tokens.append(tokenizer.cls_token)\n",
        "    ## Add Tokens and separators\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "\n",
        "    if include_SEP_token:\n",
        "        tokens.append(tokenizer.sep_token)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    ## Input Mask \n",
        "    input_mask = [1] * len(input_ids)\n",
        "    ## Zero-pad sequence lenght\n",
        "    if zero_pad:\n",
        "        while len(input_ids) < max_seq_length:\n",
        "            input_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "    return torch.tensor(input_ids).unsqueeze(0), input_mask"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM4hAccz7YzB"
      },
      "source": [
        "\"\"\"\n",
        "Get Reply\n",
        "\"\"\"\n",
        "\n",
        "def get_reply(msg):\n",
        "  model.eval()\n",
        "  input_msg, _ = prepare_features(msg)\n",
        "  if torch.cuda.is_available():\n",
        "    #Send Input Message to CUDA\n",
        "    #Line - 1\n",
        "    sent = input_msg.cuda()\n",
        "  #Use the model to generate probability vector  \n",
        "  #Line - 2\n",
        "  output = model.forward(sent)[0]\n",
        "  #print(output)\n",
        "\n",
        "  #Use torch.max function to return the prediction index with highest probability. Check how to use torch.max?\n",
        "  #Line - 3\n",
        "  _, predicted = torch.max(output, 1)\n",
        "\n",
        "  #Match prediction index to actual prediction label. Use the label_to_ix = {} dictionary we created earlier.\n",
        "  #Line - 4\n",
        "  for key, value in label_to_ix.items():\n",
        "    if predicted == value:\n",
        "        prediction = key\n",
        "\n",
        "  return prediction"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "jAxf6zfU7grI",
        "outputId": "1d770b15-97e0-4dd4-cca1-212885da9047"
      },
      "source": [
        "\"\"\"\n",
        "Test your function on sentences\n",
        "\"\"\"\n",
        "get_reply(input(\"Enter your sentence: \"))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter your sentence: be in livingroom right now\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'goToRoom'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}